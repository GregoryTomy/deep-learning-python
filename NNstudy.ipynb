{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "### Matrix shapes\n",
    "The input matrix will have rows that correspond to the number of samples. The columns will correspond to the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. ,  3. ,  2.5],\n",
       "       [ 2. ,  5. , -1. ,  2. ],\n",
       "       [-1.5,  2.7,  3.3, -0.8]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "         [2.0, 5.0, -1.0, 2.0],\n",
    "         [-1.5, 2.7, 3.3, -0.8]]\n",
    "inputs = np.array(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights will have rows that correspond to the number of neurons. The columns will correspond to the number of features in the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "weights = np.array(weights)\n",
    "biases = np.array(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which parameter to put first?\n",
    "We have the inputs as first parameter of the matrix product as the result will consist of a list of layer outputs per each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = inputs @ weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer Class\n",
    "\n",
    "A dense layer, also known as a fully-connected layer.\n",
    "\n",
    "Note: we initalize weigghts as (features, n_neurons) to skip transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    A layer of neurons.\n",
    "    features: number of input features\n",
    "    n_neurons: number of neurons in a layer\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, n_neurons: int):\n",
    "        #Initalize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(features, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs: ndarray):\n",
    "        \"\"\"Calculate output values from inputs, weights and biases\"\"\"\n",
    "        self.output = inputs @ self.weights + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.04751874e-04  1.13953603e-04 -4.79834998e-05]\n",
      " [-2.74148420e-04  3.17291502e-04 -8.69217984e-05]\n",
      " [-4.21883655e-04  5.26662567e-04 -5.59126856e-05]\n",
      " [-5.77076804e-04  7.14014051e-04 -8.94304394e-05]]\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "\n",
    "# create data set with 3 classes, 2 input features and \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# testing dense layer with 2 input features and 3 neurons (output values)\n",
    "dense_1 = Dense(2, 3)\n",
    "\n",
    "# perform forward pass of training data through the layer\n",
    "dense_1.forward(X)\n",
    "print(dense_1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "## Rectified Linear Units (ReLU)\n",
    "\n",
    "$$f(x)= max(0, x) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Units activation function\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# testing the activation class\n",
    "activation = ActivationReLU()\n",
    "activation.forward(dense_1.output)\n",
    "print(activation.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function\n",
    "$$S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^L e^{z_{i,l}}}$$\n",
    "where index $i$ means the current sample and the index $j$ means the current output in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values: [121.51041752   3.35348465  10.85906266]\n",
      "normalized exponential values: [0.89528266 0.02470831 0.08000903]\n"
     ]
    }
   ],
   "source": [
    "# example to understand softmax\n",
    "ex_layer_outputs = [4.8, 1.21, 2.385]\n",
    "# for each value in vector, calculate the exponential value\n",
    "exp_values = np.exp(ex_layer_outputs)\n",
    "print(f'exponentiated values: {exp_values}')\n",
    "\n",
    "# normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(f'normalized exponential values: {norm_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train in batches, we need to convert the function to accept layer outputs in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis: 18.172\n",
      "Sum without axis: [15.11   0.451  2.611]\n",
      "Sum without axis: [8.395 7.29  2.487]\n",
      "Sum without axis: [[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "ex_layer_outputs_2 = np.array([[ 4.8 , 1.21 , 2.385 ],\n",
    "                               [ 8.9 , - 1.81 , 0.2 ],\n",
    "                               [ 1.41 , 1.051 , 0.026 ]])\n",
    "ex_layer_outputs_2\n",
    "\n",
    "# Brief review of np.sum\n",
    "## sum without axis sums up all the values\n",
    "print(f'Sum without axis: {np.sum(ex_layer_outputs_2)}')\n",
    "\n",
    "## sum axis=0 will give u sum of the columns\n",
    "print(f'Sum without axis: {np.sum(ex_layer_outputs_2, axis=0)}')\n",
    "\n",
    "## sum axis=1 will give u sum of the rows\n",
    "print(f'Sum without axis: {np.sum(ex_layer_outputs_2, axis=1)}')\n",
    "\n",
    "## keep dim will output a single value per sample\n",
    "print(f'Sum without axis: {np.sum(ex_layer_outputs_2, axis=1, keepdims=True)}')\n",
    "\n",
    "# exp_values = np.exp(ex_layer_outputs)\n",
    "# probas =exp_values / np.sum(exp_values, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handiling exploding values\n",
    "Softmax activation is a source of \"exploding\" values, due to the nature of exponential functions. In order to prevet overflows, we subtract the maximum value from a list of inputs. This would change the output values to be in a range from some negative value to 0 (max minus itself is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSoftmax:\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        # get exp values whilst preventing overflow\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        #normalize for each sample\n",
    "        probabs = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# first layer with 3 neurons\n",
    "dense_1 = Dense(2, 3)\n",
    "\n",
    "# ReLU activation for first layer\n",
    "activation_1 = ActivationReLU()\n",
    "\n",
    "# second layer with 3 input feautures (input features in the second layer correspond to the outputs from the first layer)\n",
    "# and 3 neurons\n",
    "dense_2 = Dense(3, 3)\n",
    "\n",
    "# softmax activagtion for second layer\n",
    "activation_2 =  ActivationSoftmax()\n",
    "\n",
    "# forward pass through the network\n",
    "dense_1.forward(X)\n",
    "activation_1.forward(dense_1.output) #inputs to the activation function will be the output from the first dense layer\n",
    "dense_2.forward(activation_1.output)\n",
    "activation_2.forward(dense_2.output)\n",
    "\n",
    "print(activation_2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caclulating Network Error with Loss\n",
    "## Categorical Cross-Entropy Loss\n",
    "$$L_i = - \\sum_j y_i \\log(\\hat{y_{i, j}})$$\n",
    "Where $L_i$ denote the sample loss value, \n",
    "\n",
    "$i$ is the i-th sample in the set, \n",
    "\n",
    "$j$ is the label/output index,\n",
    "\n",
    "$y$ is the target values \n",
    "\n",
    "and $\\hat y$ denotes the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of confidences at the target indices for each sample: [0.7 0.5 0.9]\n",
      "negative log of the confidences: [0.35667494 0.69314718 0.10536052]\n",
      "average loss: 0.39\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "                            [ 0.1 , 0.5 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "class_tragets = [0, 1, 1]\n",
    "confidences = softmax_outputs[range(len(softmax_outputs)), class_tragets]\n",
    "neg_log = -np.log(confidences)\n",
    "average_loss = np.mean(neg_log)\n",
    "print(f'list of confidences at the target indices for each sample: {confidences}')\n",
    "print(f'negative log of the confidences: {neg_log}')\n",
    "print(f'average loss: {average_loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifics of the categorical cross entropy function\n",
    "The cat-cross entropy function handles targets both as sparse (contain correct class numbers) and as one-hot encoded vectors. The check is performed by counting dimensions - if targets are 1D (like a list) they are sparce, but if they are 2d (like a list of lists), then they are sets of one-hot encoded vectors.\n",
    "\n",
    "Furthermore, it is possible that the model will have full confidence for one label. It is also possible that the model will assign full confidence to a value that wasn't the target. Calculating the loss of this 0 confidence will give us an error (log(0) is undefined).\n",
    "\n",
    "To solve this, we add a very small value to the confidences. However, this introduces two new issues. First, when the confidence is 1, the new value will be slighlty larger than one resulting in a negative value instead of being zero - $-np.log(1+1e-7)$. Second, a confidence can be shifted towards 1. To prevent both these issues, we clip values from both sides by the same number. Thus, the lowest possible value will become $1+1e-7$ and the highest possible value will become $1-1e-7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    Base loss class that calculatges the data and regularizes losses given model\n",
    "    output and ground truth values.\n",
    "    \"\"\"\n",
    "    def calculate(self, output:ndarray, y:ndarray) -> float:\n",
    "        loss = np.mean(self.forward(output, y))\n",
    "        \n",
    "        return  loss\n",
    "        \n",
    "class CatCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # num of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip data on both sides\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # calculate probabilities for target values\n",
    "        # if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            confs = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        # mask values - if one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            confs = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "            \n",
    "        neg_log_likelihoods = -np.log(confs)\n",
    "        \n",
    "        return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss spares: 0.38506088005216804\n",
      "Loss ohe: 0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "                            [ 0.1 , 0.5 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "targets_sparse = np.array([0, 1, 1])\n",
    "targets_ohe = np.array([[ 1 , 0 , 0 ],\n",
    "                        [ 0 , 1 , 0 ],\n",
    "                        [ 0 , 1 , 0 ]])\n",
    "\n",
    "loss_func = CatCrossEntropy()\n",
    "loss_sparse= loss_func.calculate(softmax_outputs, targets_sparse)\n",
    "loss_ohe = loss_func.calculate(softmax_outputs, targets_ohe)\n",
    "\n",
    "print(f'Loss spares: {loss_sparse}')\n",
    "print(f'Loss ohe: {loss_ohe}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.10\n"
     ]
    }
   ],
   "source": [
    "# continuing the testing\n",
    "# output from the second layer\n",
    "# activation_2.output\n",
    "loss = loss_func.calculate(activation_2.output, y)\n",
    "print(f'Loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "In common practive, accuracy is used along with loss. Accuracy describes how often the largest confidence is the correct class in terms of a fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy from the output of activation_2 and targets\n",
    "preds = np.argmax(activation_2.output, axis=1)\n",
    "# if targets are OHE then convert them\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(preds == y)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Introduce backward method that will carry out backpropagation.\n",
    "\n",
    "1. We will want to remember the inputs to calculate the partial derivatives wrt weights during back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([[ 1 , 2 , - 3 , - 4 ],\n",
    "              [ 2 , - 7 , - 1 , 3 ],\n",
    "              [ - 1 , 2 , 5 , - 1 ]])\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z>0] = 1\n",
    "drelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    A layer of neurons.\n",
    "    features: number of input features\n",
    "    n_neurons: number of neurons in a layer\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, n_neurons: int):\n",
    "        #Initalize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(features, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs: ndarray):\n",
    "        \"\"\"Calculate output values from inputs, weights and biases\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs @ self.weights + self.biases\n",
    "        \n",
    "    def backward(self, dvalues:ndarray):\n",
    "        \"\"\"Calclates the gradient on parameters and values\n",
    "\n",
    "        Args:\n",
    "            dvalues (ndarray): passed in gradient from the next layer\n",
    "        \"\"\"\n",
    "        self.dweights = self.inputs.T @ dvalues\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = dvalues @ self.weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Units activation function\n",
    "    \"\"\"\n",
    "    def forward(self, inputs: ndarray):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues: ndarray):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We turn numerical labels into OHE vectors.\n",
    "2. Normalize the CCE gradient. Since optimizers sum all of the gradients related to each weight and bias before multiplying them by the learning rate, this sum will increase the more samples we have. Thus, we will ahve to adjust the learning rate according to each set of samples. Instead, we normalize the gradient by taking the mean, making their sum's magnitude invarient to the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    Base loss class that calculatges the data and regularizes losses given model\n",
    "    output and ground truth values.\n",
    "    \"\"\"\n",
    "    def calculate(self, output:ndarray, y:ndarray) -> float:\n",
    "        loss = np.mean(self.forward(output, y))\n",
    "        \n",
    "        return  loss\n",
    "        \n",
    "class CatCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # num of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip data on both sides\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # calculate probabilities for target values\n",
    "        # if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            confs = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        # mask values - if one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            confs = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "            \n",
    "        neg_log_likelihoods = -np.log(confs)\n",
    "        \n",
    "        return neg_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues: ndarray, y_true:ndarray):\n",
    "        samples, labels = dvalues.shape\n",
    "        # if sparse labels, convert to OHE vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # calclate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # noramlize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative: Softmax activation function\n",
    "The derivative to the softmax activation function can be calculated as:\n",
    "$$\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} = S_{i,j}.(\\delta_{j,k} - S_{i,k})$$ \n",
    "where the delta denotes the Kronecker function. Proof is omitted here.\n",
    "\n",
    "### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n",
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n",
      "[[ 0.21 -0.07 -0.14]\n",
      " [-0.07  0.09 -0.02]\n",
      " [-0.14 -0.02  0.16]]\n"
     ]
    }
   ],
   "source": [
    "softmax_output = np.array([0.7, 0.1, 0.2]).reshape(-1,1)\n",
    "softmax_output\n",
    "\n",
    "# the left side of the equation (when distributed) is the Softmax's output multiplied by the Kronecker delta,\n",
    "# which quals 1 when both inputs are equal ad 0 otherwise. When visualized as an array,\n",
    "# we'll have an identity matrix\n",
    "\n",
    "# help(np.diagflat)\n",
    "first = np.diagflat(softmax_output)\n",
    "print(first)\n",
    "\n",
    "# the right part of the equation is the product of the softmax outputs, iterating over the\n",
    "# j and k indices respectively\n",
    "\n",
    "second = softmax_output @ softmax_output.T\n",
    "print(second)\n",
    "\n",
    "# finally, to get the derivative\n",
    "print(first - second) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSoftmax:\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    \"\"\"\n",
    "    def forward(self, inputs: ndarray):\n",
    "        # get exp values whilst preventing overflow\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        #normalize for each sample\n",
    "        probabs = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabs\n",
    "        \n",
    "    def backward(self, dvalues: ndarray):\n",
    "        # create unintialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # enumerate outputs and gradients\n",
    "        for ix,(single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "        # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate the Jacobian matrix of the output\n",
    "            # j_matrix = np.diagflat(single_output) - (single_output @ single_output.T)\n",
    "            j_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # calculate sample-wise gradient and add to array\n",
    "            # self.dinputs[ix] = j_matrix @ single_dvalues\n",
    "            self.dinputs[ix] = np.dot(j_matrix, single_dvalues)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Softmax and Cross-Entropy\n",
    "So far, we have calculated the partial derivatives of the CCE and softmax. However, there is a way to speed things up (ew loops). The derivatives of both functions combine to solve a simple equation. Proof omitted here.\n",
    "\n",
    "Applying the chain rule to calculate the partial derivative of the CCE w.r.t. the softmax inputs gives us:\n",
    "$$\\frac{\\partial L_{i}}{\\partial z_{i,k}} = \\hat y_{i,k} - y_{i,k}$$\n",
    "\n",
    "Isn't she pretty?\n",
    "\n",
    "Note: to implement the solution, instead of performing the subtraction of the full arrays, we take advantage of the fact that y-true consists of OHE vectors. Thus, there is only a singular value of 1 in these vectors. This means we can index the prediction array with the sample number and its true value index, subtractging 1 from these. This requires discrete true labels and we force this in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[ 1 , 0 , 0 ],[ 0 , 0 , 1 ],[ 0 , 1 , 0 ]])\n",
    "print(np.argmax(y_true))\n",
    "print(np.argmax(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCCE():\n",
    "    \"\"\"\n",
    "    Calculates the parital derivative of the CCE w.r.t. the inputs of the softmax activation\n",
    "    function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.activation = ActivationSoftmax()\n",
    "        self.loss = CatCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs: ndarray, y_true: ndarray):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        # if labels are OHE, turn to discrete\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47619048 -0.         -0.        ]\n",
      " [-0.         -0.66666667 -0.        ]\n",
      " [-0.         -0.37037037 -0.        ]]\n",
      "Gradients: combined loss and activation: [[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: separate loss and activation: [[-0.09999999  0.03333334  0.06666667]\n",
      " [ 0.03333334 -0.16666667  0.13333334]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# solution awesome\n",
    "softmax_loss = SoftmaxCCE()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues_1 = softmax_loss.dinputs\n",
    "\n",
    "# solution okay\n",
    "activation = ActivationSoftmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = CatCrossEntropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "print(loss.dinputs)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues_2 = activation.dinputs\n",
    "\n",
    "print(f'Gradients: combined loss and activation: {dvalues_1}')\n",
    "print(f'Gradients: separate loss and activation: {dvalues_2}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued: Calculating loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we added backward methods, we will have to reinitialze the layers\n",
    "nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# first layer with 3 neurons\n",
    "dense_1 = Dense(2, 3)\n",
    "\n",
    "# ReLU activation for first layer\n",
    "activation_1 = ActivationReLU()\n",
    "\n",
    "# second layer with 3 input feautures (input features in the second layer correspond to the outputs from the first layer)\n",
    "# and 3 neurons\n",
    "dense_2 = Dense(3, 3)\n",
    "\n",
    "# softmax activagtion for second layer\n",
    "loss_activation = SoftmaxCCE()\n",
    "\n",
    "# forward pass through the network\n",
    "dense_1.forward(X)\n",
    "activation_1.forward(dense_1.output) #\n",
    "dense_2.forward(activation_1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986104\n",
      "[[ 1.57663686e-04  7.83686628e-05  4.73244181e-05]\n",
      " [ 1.81610390e-04  1.10455085e-05 -3.30963376e-05]]\n",
      "\n",
      "\n",
      "[[-3.60553531e-04  9.66117223e-05 -1.03671344e-04]]\n",
      "\n",
      "\n",
      "[[ 5.44108079e-05  1.07411492e-04 -1.61822391e-04]\n",
      " [-4.07912230e-05 -7.16780778e-05  1.12469665e-04]\n",
      " [-5.30113066e-05  8.58172934e-05 -3.28059614e-05]]\n",
      "\n",
      "\n",
      "[[-1.0732794e-05 -9.4590941e-06  2.0027626e-05]]\n"
     ]
    }
   ],
   "source": [
    "# perform forward pass through the actrivation loss function\n",
    "# takes the output of the second dense layer and returns loss\n",
    "loss = loss_activation.forward(dense_2.output, y)\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense_2.backward(loss_activation.dinputs)\n",
    "activation_1.backward(dense_2.dinputs)\n",
    "dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "print(dense_1.dweights)\n",
    "print('\\n')\n",
    "print(dense_1.dbiases)\n",
    "print('\\n')\n",
    "print(dense_2.dweights)\n",
    "print('\\n')\n",
    "print(dense_2.dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocGradDescent:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__ ( self , learning_rate = 1.0 ):\n",
    "        self.learning_rate = learning_rate\n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        layer.weights -= self.learning_rate * layer.dweights\n",
    "        layer.biases -= self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy: 0.360, loss: 1.099\n",
      "epoch: 100, accuracy: 0.400, loss: 1.087\n",
      "epoch: 200, accuracy: 0.417, loss: 1.077\n",
      "epoch: 300, accuracy: 0.413, loss: 1.076\n",
      "epoch: 400, accuracy: 0.400, loss: 1.074\n",
      "epoch: 500, accuracy: 0.400, loss: 1.071\n",
      "epoch: 600, accuracy: 0.417, loss: 1.067\n",
      "epoch: 700, accuracy: 0.437, loss: 1.062\n",
      "epoch: 800, accuracy: 0.457, loss: 1.055\n",
      "epoch: 900, accuracy: 0.410, loss: 1.062\n",
      "epoch: 1000, accuracy: 0.403, loss: 1.059\n",
      "epoch: 1100, accuracy: 0.403, loss: 1.057\n",
      "epoch: 1200, accuracy: 0.397, loss: 1.064\n",
      "epoch: 1300, accuracy: 0.433, loss: 1.051\n",
      "epoch: 1400, accuracy: 0.390, loss: 1.099\n",
      "epoch: 1500, accuracy: 0.477, loss: 1.044\n",
      "epoch: 1600, accuracy: 0.407, loss: 1.060\n",
      "epoch: 1700, accuracy: 0.410, loss: 1.037\n",
      "epoch: 1800, accuracy: 0.400, loss: 1.036\n",
      "epoch: 1900, accuracy: 0.403, loss: 1.039\n",
      "epoch: 2000, accuracy: 0.433, loss: 1.046\n",
      "epoch: 2100, accuracy: 0.500, loss: 1.014\n",
      "epoch: 2200, accuracy: 0.470, loss: 1.042\n",
      "epoch: 2300, accuracy: 0.470, loss: 1.013\n",
      "epoch: 2400, accuracy: 0.493, loss: 0.999\n",
      "epoch: 2500, accuracy: 0.450, loss: 0.998\n",
      "epoch: 2600, accuracy: 0.493, loss: 0.991\n",
      "epoch: 2700, accuracy: 0.547, loss: 0.989\n",
      "epoch: 2800, accuracy: 0.497, loss: 0.997\n",
      "epoch: 2900, accuracy: 0.510, loss: 0.977\n",
      "epoch: 3000, accuracy: 0.527, loss: 0.971\n",
      "epoch: 3100, accuracy: 0.477, loss: 0.995\n",
      "epoch: 3200, accuracy: 0.520, loss: 0.970\n",
      "epoch: 3300, accuracy: 0.527, loss: 0.981\n",
      "epoch: 3400, accuracy: 0.563, loss: 0.994\n",
      "epoch: 3500, accuracy: 0.487, loss: 0.975\n",
      "epoch: 3600, accuracy: 0.497, loss: 0.970\n",
      "epoch: 3700, accuracy: 0.517, loss: 0.977\n",
      "epoch: 3800, accuracy: 0.540, loss: 0.995\n",
      "epoch: 3900, accuracy: 0.540, loss: 0.967\n",
      "epoch: 4000, accuracy: 0.537, loss: 1.002\n",
      "epoch: 4100, accuracy: 0.490, loss: 0.970\n",
      "epoch: 4200, accuracy: 0.483, loss: 0.968\n",
      "epoch: 4300, accuracy: 0.510, loss: 0.985\n",
      "epoch: 4400, accuracy: 0.517, loss: 0.979\n",
      "epoch: 4500, accuracy: 0.567, loss: 0.973\n",
      "epoch: 4600, accuracy: 0.543, loss: 0.973\n",
      "epoch: 4700, accuracy: 0.510, loss: 0.972\n",
      "epoch: 4800, accuracy: 0.523, loss: 0.967\n",
      "epoch: 4900, accuracy: 0.550, loss: 0.983\n",
      "epoch: 5000, accuracy: 0.557, loss: 0.962\n",
      "epoch: 5100, accuracy: 0.563, loss: 0.983\n",
      "epoch: 5200, accuracy: 0.500, loss: 0.969\n",
      "epoch: 5300, accuracy: 0.527, loss: 0.974\n",
      "epoch: 5400, accuracy: 0.530, loss: 0.976\n",
      "epoch: 5500, accuracy: 0.560, loss: 0.975\n",
      "epoch: 5600, accuracy: 0.497, loss: 0.952\n",
      "epoch: 5700, accuracy: 0.517, loss: 0.962\n",
      "epoch: 5800, accuracy: 0.533, loss: 0.969\n",
      "epoch: 5900, accuracy: 0.557, loss: 0.957\n",
      "epoch: 6000, accuracy: 0.550, loss: 0.951\n",
      "epoch: 6100, accuracy: 0.583, loss: 0.999\n",
      "epoch: 6200, accuracy: 0.530, loss: 0.938\n",
      "epoch: 6300, accuracy: 0.537, loss: 0.960\n",
      "epoch: 6400, accuracy: 0.517, loss: 0.937\n",
      "epoch: 6500, accuracy: 0.550, loss: 0.963\n",
      "epoch: 6600, accuracy: 0.630, loss: 0.939\n",
      "epoch: 6700, accuracy: 0.540, loss: 0.944\n",
      "epoch: 6800, accuracy: 0.570, loss: 0.961\n",
      "epoch: 6900, accuracy: 0.563, loss: 0.922\n",
      "epoch: 7000, accuracy: 0.560, loss: 0.920\n",
      "epoch: 7100, accuracy: 0.540, loss: 0.928\n",
      "epoch: 7200, accuracy: 0.577, loss: 0.884\n",
      "epoch: 7300, accuracy: 0.567, loss: 0.946\n",
      "epoch: 7400, accuracy: 0.613, loss: 0.865\n",
      "epoch: 7500, accuracy: 0.577, loss: 0.880\n",
      "epoch: 7600, accuracy: 0.547, loss: 0.884\n",
      "epoch: 7700, accuracy: 0.637, loss: 0.850\n",
      "epoch: 7800, accuracy: 0.600, loss: 0.878\n",
      "epoch: 7900, accuracy: 0.583, loss: 0.931\n",
      "epoch: 8000, accuracy: 0.633, loss: 0.850\n",
      "epoch: 8100, accuracy: 0.627, loss: 0.833\n",
      "epoch: 8200, accuracy: 0.617, loss: 0.850\n",
      "epoch: 8300, accuracy: 0.593, loss: 0.864\n",
      "epoch: 8400, accuracy: 0.580, loss: 0.891\n",
      "epoch: 8500, accuracy: 0.563, loss: 0.863\n",
      "epoch: 8600, accuracy: 0.617, loss: 0.858\n",
      "epoch: 8700, accuracy: 0.633, loss: 0.849\n",
      "epoch: 8800, accuracy: 0.617, loss: 0.876\n",
      "epoch: 8900, accuracy: 0.620, loss: 0.900\n",
      "epoch: 9000, accuracy: 0.637, loss: 0.857\n",
      "epoch: 9100, accuracy: 0.610, loss: 0.835\n",
      "epoch: 9200, accuracy: 0.617, loss: 0.863\n",
      "epoch: 9300, accuracy: 0.593, loss: 0.904\n",
      "epoch: 9400, accuracy: 0.563, loss: 0.856\n",
      "epoch: 9500, accuracy: 0.587, loss: 0.867\n",
      "epoch: 9600, accuracy: 0.617, loss: 0.860\n",
      "epoch: 9700, accuracy: 0.620, loss: 0.849\n",
      "epoch: 9800, accuracy: 0.623, loss: 0.892\n",
      "epoch: 9900, accuracy: 0.613, loss: 0.883\n",
      "epoch: 10000, accuracy: 0.637, loss: 0.846\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "# test\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# set up the neural network\n",
    "dense_1 = Dense(2, 64)\n",
    "activation_1 = ActivationReLU()\n",
    "dense_2 = Dense(64, 3)\n",
    "loss_activation = SoftmaxCCE()\n",
    "optim = StocGradDescent()\n",
    "# training over epochs\n",
    "for epoch in range(10001):\n",
    "    # perform forward pass\n",
    "    dense_1.forward(X)\n",
    "    activation_1.forward(dense_1.output)\n",
    "    dense_2.forward(activation_1.output)\n",
    "    loss = loss_activation.forward(dense_2.output, y)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(preds == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, accuracy: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "    # perform backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense_2.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(dense_2.dinputs)\n",
    "    dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "    # update weights and biases\n",
    "    optim.update_params(dense_1)\n",
    "    optim.update_params(dense_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "The idea of a learning rate decay is to start with a large learning rate and then decrease it during trainig. We will use a 1/t decaying or exponential decaying rate. Initially, the lr drops fast, but the change in lr lowers with each step.\n",
    "\n",
    "Note: the 1 + learning rate ensures that the result is always a fraction of the starting learning rate i.e. will be always less than or equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "1.000\n",
      "0.909\n",
      "0.833\n",
      "0.769\n",
      "0.714\n",
      "0.667\n",
      "0.625\n",
      "0.588\n",
      "0.556\n",
      "0.526\n",
      "0.500\n",
      "0.476\n",
      "0.455\n",
      "0.435\n",
      "0.417\n",
      "0.400\n",
      "0.385\n",
      "0.370\n",
      "0.357\n",
      "0.345\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = 1\n",
    "decay_rate = 0.1\n",
    "step = 1\n",
    "\n",
    "learning_rate = init_learning_rate * (1 / (1 + decay_rate * step))\n",
    "\n",
    "print(learning_rate)\n",
    "\n",
    "for step in range(20):\n",
    "    learning_rate = init_learning_rate * (1 / (1 + decay_rate * step))\n",
    "    print(f'{learning_rate:.3f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocGradDescent:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__ ( self , learning_rate: float=1.0, decay: float=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_lr = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        # if self.decay:\n",
    "        #     self.current_lr = self.learning_rate * \\\n",
    "        #         (1. / (1. + self.decay * self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer:Dense):\n",
    "        layer.weights -= self.current_lr * layer.dweights\n",
    "        layer.biases -= self.current_lr * layer.dbiases\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        \"\"\"\n",
    "        Updates self.current_lr if decay rate is non-zero.\n",
    "        Call once before any paramter updates.\n",
    "        \"\"\"\n",
    "        if self.decay:\n",
    "            self.current_lr = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_iter(self):\n",
    "        \"\"\"Tracks iterations by updating self.iterations\n",
    "        \"\"\"\n",
    "        self.iterations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy: 0.360,               loss: 1.099, lr: 1.000\n",
      "epoch: 1000, accuracy: 0.443,               loss: 1.063, lr: 0.500\n",
      "epoch: 2000, accuracy: 0.463,               loss: 0.999, lr: 0.333\n",
      "epoch: 3000, accuracy: 0.490,               loss: 0.990, lr: 0.250\n",
      "epoch: 4000, accuracy: 0.533,               loss: 0.947, lr: 0.200\n",
      "epoch: 5000, accuracy: 0.577,               loss: 0.902, lr: 0.167\n",
      "epoch: 6000, accuracy: 0.630,               loss: 0.860, lr: 0.143\n",
      "epoch: 7000, accuracy: 0.673,               loss: 0.824, lr: 0.125\n",
      "epoch: 8000, accuracy: 0.643,               loss: 0.799, lr: 0.111\n",
      "epoch: 9000, accuracy: 0.640,               loss: 0.783, lr: 0.100\n",
      "epoch: 10000, accuracy: 0.670,               loss: 0.771, lr: 0.091\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "# test\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# set up the neural network\n",
    "dense_1 = Dense(2, 64)\n",
    "activation_1 = ActivationReLU()\n",
    "dense_2 = Dense(64, 3)\n",
    "loss_activation = SoftmaxCCE()\n",
    "optim = StocGradDescent(decay=1e-3)\n",
    "# training over epochs\n",
    "for epoch in range(10001):\n",
    "    # perform forward pass\n",
    "    dense_1.forward(X)\n",
    "    activation_1.forward(dense_1.output)\n",
    "    dense_2.forward(activation_1.output)\n",
    "    loss = loss_activation.forward(dense_2.output, y)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(preds == y)\n",
    "\n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, accuracy: {accuracy:.3f}, \\\n",
    "              loss: {loss:.3f}, lr: {optim.current_lr:.3f}')\n",
    "    # perform backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense_2.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(dense_2.dinputs)\n",
    "    dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "    # update weights and biases\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(dense_1)\n",
    "    optim.update_params(dense_2)\n",
    "    optim.update_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "Momentum creates a rolling average of gradients over some number of updates and uses this average with the uniquw gradient at each step. With momentum, a model is more likely to pass through local minimums, further decreasing loss.\n",
    "\n",
    "We set a parameter between 0 and 1, representing the fraction of the previous parameter update to retain, and subtracting our actual gradient multipplied by the learning rate, from it. The update contains a portion of the gradient from the preceding steps as our momentum and only a portion of the current gradient. Together, these portions form the actual change to our parameters.\n",
    "\n",
    "The bigger the role that momentum takes in the update, the slower the update can change the direction.\n",
    "\n",
    "weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "(self.current_learning_rate * layer.dweights) \n",
    "\n",
    "The hyper-parameter self.momentum is chosen at the start and the layer.weight_momentums starts as all zeros but update during training.\n",
    "\n",
    "### SGD Optimizer with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocGradDescent:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__ ( self , learning_rate: float=1.0, decay: float=0.,\n",
    "                  momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_lr = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        # if self.decay:\n",
    "        #     self.current_lr = self.learning_rate * \\\n",
    "        #         (1. / (1. + self.decay * self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer:Dense):\n",
    "        # if using momentum\n",
    "        if self.momentum:\n",
    "            # create momentum arrays for layer if layer\n",
    "            # does not contain it.\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # take prev updates times retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums + \\\n",
    "                self.current_lr * layer.dweights\n",
    "            layer.weight_momentums = weight_updates        \n",
    "            \n",
    "            bias_updates = self.momentum * layer.bias_momentums + \\\n",
    "                self.current_lr * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates        \n",
    "        \n",
    "        else:   \n",
    "            weight_updates = self.current_lr * layer.dweights\n",
    "            bias_updates = self.current_lr * layer.dbiases\n",
    "        \n",
    "        # update weights and biases\n",
    "        layer.weights -= weight_updates\n",
    "        layer.biases -= bias_updates\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        \"\"\"\n",
    "        Updates self.current_lr if decay rate is non-zero.\n",
    "        Call once before any paramter updates.\n",
    "        \"\"\"\n",
    "        if self.decay:\n",
    "            self.current_lr = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_iter(self):\n",
    "        \"\"\"Tracks iterations by updating self.iterations\n",
    "        \"\"\"\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy: 0.360,               loss: 1.099, lr: 1.000\n",
      "epoch: 100, accuracy: 0.443,               loss: 1.053, lr: 0.910\n",
      "epoch: 200, accuracy: 0.543,               loss: 0.972, lr: 0.834\n",
      "epoch: 300, accuracy: 0.657,               loss: 0.763, lr: 0.770\n",
      "epoch: 400, accuracy: 0.727,               loss: 0.676, lr: 0.715\n",
      "epoch: 500, accuracy: 0.787,               loss: 0.571, lr: 0.667\n",
      "epoch: 600, accuracy: 0.807,               loss: 0.500, lr: 0.625\n",
      "epoch: 700, accuracy: 0.823,               loss: 0.434, lr: 0.589\n",
      "epoch: 800, accuracy: 0.830,               loss: 0.435, lr: 0.556\n",
      "epoch: 900, accuracy: 0.840,               loss: 0.412, lr: 0.527\n",
      "epoch: 1000, accuracy: 0.850,               loss: 0.388, lr: 0.500\n",
      "epoch: 1100, accuracy: 0.873,               loss: 0.346, lr: 0.476\n",
      "epoch: 1200, accuracy: 0.857,               loss: 0.369, lr: 0.455\n",
      "epoch: 1300, accuracy: 0.873,               loss: 0.327, lr: 0.435\n",
      "epoch: 1400, accuracy: 0.877,               loss: 0.321, lr: 0.417\n",
      "epoch: 1500, accuracy: 0.887,               loss: 0.309, lr: 0.400\n",
      "epoch: 1600, accuracy: 0.887,               loss: 0.294, lr: 0.385\n",
      "epoch: 1700, accuracy: 0.887,               loss: 0.276, lr: 0.371\n",
      "epoch: 1800, accuracy: 0.890,               loss: 0.264, lr: 0.357\n",
      "epoch: 1900, accuracy: 0.890,               loss: 0.252, lr: 0.345\n",
      "epoch: 2000, accuracy: 0.893,               loss: 0.242, lr: 0.333\n",
      "epoch: 2100, accuracy: 0.913,               loss: 0.248, lr: 0.323\n",
      "epoch: 2200, accuracy: 0.910,               loss: 0.231, lr: 0.313\n",
      "epoch: 2300, accuracy: 0.913,               loss: 0.227, lr: 0.303\n",
      "epoch: 2400, accuracy: 0.897,               loss: 0.227, lr: 0.294\n",
      "epoch: 2500, accuracy: 0.930,               loss: 0.219, lr: 0.286\n",
      "epoch: 2600, accuracy: 0.927,               loss: 0.216, lr: 0.278\n",
      "epoch: 2700, accuracy: 0.913,               loss: 0.214, lr: 0.270\n",
      "epoch: 2800, accuracy: 0.917,               loss: 0.210, lr: 0.263\n",
      "epoch: 2900, accuracy: 0.907,               loss: 0.211, lr: 0.256\n",
      "epoch: 3000, accuracy: 0.937,               loss: 0.203, lr: 0.250\n",
      "epoch: 3100, accuracy: 0.923,               loss: 0.200, lr: 0.244\n",
      "epoch: 3200, accuracy: 0.923,               loss: 0.198, lr: 0.238\n",
      "epoch: 3300, accuracy: 0.923,               loss: 0.195, lr: 0.233\n",
      "epoch: 3400, accuracy: 0.923,               loss: 0.193, lr: 0.227\n",
      "epoch: 3500, accuracy: 0.923,               loss: 0.191, lr: 0.222\n",
      "epoch: 3600, accuracy: 0.927,               loss: 0.190, lr: 0.217\n",
      "epoch: 3700, accuracy: 0.923,               loss: 0.188, lr: 0.213\n",
      "epoch: 3800, accuracy: 0.923,               loss: 0.186, lr: 0.208\n",
      "epoch: 3900, accuracy: 0.927,               loss: 0.185, lr: 0.204\n",
      "epoch: 4000, accuracy: 0.927,               loss: 0.183, lr: 0.200\n",
      "epoch: 4100, accuracy: 0.927,               loss: 0.181, lr: 0.196\n",
      "epoch: 4200, accuracy: 0.927,               loss: 0.180, lr: 0.192\n",
      "epoch: 4300, accuracy: 0.930,               loss: 0.179, lr: 0.189\n",
      "epoch: 4400, accuracy: 0.930,               loss: 0.178, lr: 0.185\n",
      "epoch: 4500, accuracy: 0.930,               loss: 0.177, lr: 0.182\n",
      "epoch: 4600, accuracy: 0.933,               loss: 0.176, lr: 0.179\n",
      "epoch: 4700, accuracy: 0.937,               loss: 0.175, lr: 0.175\n",
      "epoch: 4800, accuracy: 0.937,               loss: 0.175, lr: 0.172\n",
      "epoch: 4900, accuracy: 0.937,               loss: 0.174, lr: 0.170\n",
      "epoch: 5000, accuracy: 0.937,               loss: 0.173, lr: 0.167\n",
      "epoch: 5100, accuracy: 0.937,               loss: 0.172, lr: 0.164\n",
      "epoch: 5200, accuracy: 0.937,               loss: 0.172, lr: 0.161\n",
      "epoch: 5300, accuracy: 0.937,               loss: 0.171, lr: 0.159\n",
      "epoch: 5400, accuracy: 0.937,               loss: 0.170, lr: 0.156\n",
      "epoch: 5500, accuracy: 0.933,               loss: 0.170, lr: 0.154\n",
      "epoch: 5600, accuracy: 0.933,               loss: 0.169, lr: 0.152\n",
      "epoch: 5700, accuracy: 0.933,               loss: 0.169, lr: 0.149\n",
      "epoch: 5800, accuracy: 0.930,               loss: 0.168, lr: 0.147\n",
      "epoch: 5900, accuracy: 0.933,               loss: 0.168, lr: 0.145\n",
      "epoch: 6000, accuracy: 0.933,               loss: 0.167, lr: 0.143\n",
      "epoch: 6100, accuracy: 0.930,               loss: 0.167, lr: 0.141\n",
      "epoch: 6200, accuracy: 0.930,               loss: 0.166, lr: 0.139\n",
      "epoch: 6300, accuracy: 0.930,               loss: 0.166, lr: 0.137\n",
      "epoch: 6400, accuracy: 0.930,               loss: 0.166, lr: 0.135\n",
      "epoch: 6500, accuracy: 0.930,               loss: 0.165, lr: 0.133\n",
      "epoch: 6600, accuracy: 0.930,               loss: 0.165, lr: 0.132\n",
      "epoch: 6700, accuracy: 0.930,               loss: 0.164, lr: 0.130\n",
      "epoch: 6800, accuracy: 0.930,               loss: 0.164, lr: 0.128\n",
      "epoch: 6900, accuracy: 0.930,               loss: 0.164, lr: 0.127\n",
      "epoch: 7000, accuracy: 0.930,               loss: 0.163, lr: 0.125\n",
      "epoch: 7100, accuracy: 0.930,               loss: 0.163, lr: 0.123\n",
      "epoch: 7200, accuracy: 0.930,               loss: 0.163, lr: 0.122\n",
      "epoch: 7300, accuracy: 0.930,               loss: 0.162, lr: 0.120\n",
      "epoch: 7400, accuracy: 0.930,               loss: 0.162, lr: 0.119\n",
      "epoch: 7500, accuracy: 0.930,               loss: 0.162, lr: 0.118\n",
      "epoch: 7600, accuracy: 0.930,               loss: 0.161, lr: 0.116\n",
      "epoch: 7700, accuracy: 0.930,               loss: 0.161, lr: 0.115\n",
      "epoch: 7800, accuracy: 0.930,               loss: 0.161, lr: 0.114\n",
      "epoch: 7900, accuracy: 0.930,               loss: 0.161, lr: 0.112\n",
      "epoch: 8000, accuracy: 0.930,               loss: 0.160, lr: 0.111\n",
      "epoch: 8100, accuracy: 0.930,               loss: 0.160, lr: 0.110\n",
      "epoch: 8200, accuracy: 0.930,               loss: 0.160, lr: 0.109\n",
      "epoch: 8300, accuracy: 0.927,               loss: 0.159, lr: 0.108\n",
      "epoch: 8400, accuracy: 0.930,               loss: 0.159, lr: 0.106\n",
      "epoch: 8500, accuracy: 0.930,               loss: 0.159, lr: 0.105\n",
      "epoch: 8600, accuracy: 0.927,               loss: 0.159, lr: 0.104\n",
      "epoch: 8700, accuracy: 0.930,               loss: 0.159, lr: 0.103\n",
      "epoch: 8800, accuracy: 0.927,               loss: 0.158, lr: 0.102\n",
      "epoch: 8900, accuracy: 0.930,               loss: 0.158, lr: 0.101\n",
      "epoch: 9000, accuracy: 0.930,               loss: 0.158, lr: 0.100\n",
      "epoch: 9100, accuracy: 0.930,               loss: 0.158, lr: 0.099\n",
      "epoch: 9200, accuracy: 0.930,               loss: 0.157, lr: 0.098\n",
      "epoch: 9300, accuracy: 0.930,               loss: 0.157, lr: 0.097\n",
      "epoch: 9400, accuracy: 0.927,               loss: 0.157, lr: 0.096\n",
      "epoch: 9500, accuracy: 0.927,               loss: 0.157, lr: 0.095\n",
      "epoch: 9600, accuracy: 0.927,               loss: 0.157, lr: 0.094\n",
      "epoch: 9700, accuracy: 0.927,               loss: 0.156, lr: 0.093\n",
      "epoch: 9800, accuracy: 0.930,               loss: 0.156, lr: 0.093\n",
      "epoch: 9900, accuracy: 0.927,               loss: 0.156, lr: 0.092\n",
      "epoch: 10000, accuracy: 0.927,               loss: 0.156, lr: 0.091\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# set up the neural network\n",
    "dense_1 = Dense(2, 64)\n",
    "activation_1 = ActivationReLU()\n",
    "dense_2 = Dense(64, 3)\n",
    "loss_activation = SoftmaxCCE()\n",
    "optim = StocGradDescent(decay=1e-3, momentum=0.9)\n",
    "# training over epochs\n",
    "for epoch in range(10001):\n",
    "    # perform forward pass\n",
    "    dense_1.forward(X)\n",
    "    activation_1.forward(dense_1.output)\n",
    "    dense_2.forward(activation_1.output)\n",
    "    loss = loss_activation.forward(dense_2.output, y)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(preds == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, accuracy: {accuracy:.3f}, \\\n",
    "              loss: {loss:.3f}, lr: {optim.current_lr:.3f}')\n",
    "    # perform backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense_2.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(dense_2.dinputs)\n",
    "    dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "    # update weights and biases\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(dense_1)\n",
    "    optim.update_params(dense_2)\n",
    "    optim.update_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "AdaGrad or Adaptive Gradient, institues a per-parameter learning rate rahter than a globally-shared rate. AdaGrad provides a way to normalize parameter updates by keeping a history of previous updates - the bigger the sum of the updates, positive or negative, the smaller updates are made further in training.\n",
    "\n",
    "cache += parm_gradient ** 2\n",
    "parm_updates = learning_rate * parm_gradient / (sqrt(cache) + eps)\n",
    "\n",
    "The cache holds a history of the squared gradients. The parm__updates follows a vanill SGD but is then divided by the square root of the cache plus some epsilon value. Note: the division operation with a constantaly rising cache might cause the learning to stall as updates become smaller over time (monotonic nature of updates). Hence, this optimizer is not widely used, except for specific applications.\n",
    "\n",
    "Epsilon is a hyperparameter preventing division by 0. \n",
    "\n",
    "Notice also that we sum the squared value and then square root later. The resulting cache value will grow slower, taking care of negative numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__ ( self , learning_rate: float=1.0, decay: float=0.,\n",
    "                  epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_lr = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        # if self.decay:\n",
    "        #     self.current_lr = self.learning_rate * \\\n",
    "        #         (1. / (1. + self.decay * self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer:Dense):\n",
    "        # create cache arrays for layer if layer\n",
    "        # does not contain it.\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # update cachec with squared gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        layer.weights -= self.current_lr * layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases -= self.current_lr * layer.dbiases / \\\n",
    "            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        \"\"\"\n",
    "        Updates self.current_lr if decay rate is non-zero.\n",
    "        Call once before any paramter updates.\n",
    "        \"\"\"\n",
    "        if self.decay:\n",
    "            self.current_lr = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_iter(self):\n",
    "        \"\"\"Tracks iterations by updating self.iterations\n",
    "        \"\"\"\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy: 0.360,               loss: 1.099, lr: 1.000\n",
      "epoch: 100, accuracy: 0.460,               loss: 1.010, lr: 0.990\n",
      "epoch: 200, accuracy: 0.527,               loss: 0.937, lr: 0.980\n",
      "epoch: 300, accuracy: 0.607,               loss: 0.872, lr: 0.971\n",
      "epoch: 400, accuracy: 0.617,               loss: 0.825, lr: 0.962\n",
      "epoch: 500, accuracy: 0.633,               loss: 0.784, lr: 0.952\n",
      "epoch: 600, accuracy: 0.630,               loss: 0.768, lr: 0.943\n",
      "epoch: 700, accuracy: 0.647,               loss: 0.741, lr: 0.935\n",
      "epoch: 800, accuracy: 0.673,               loss: 0.727, lr: 0.926\n",
      "epoch: 900, accuracy: 0.690,               loss: 0.689, lr: 0.918\n",
      "epoch: 1000, accuracy: 0.687,               loss: 0.676, lr: 0.909\n",
      "epoch: 1100, accuracy: 0.693,               loss: 0.657, lr: 0.901\n",
      "epoch: 1200, accuracy: 0.703,               loss: 0.644, lr: 0.893\n",
      "epoch: 1300, accuracy: 0.713,               loss: 0.631, lr: 0.885\n",
      "epoch: 1400, accuracy: 0.710,               loss: 0.616, lr: 0.877\n",
      "epoch: 1500, accuracy: 0.730,               loss: 0.604, lr: 0.870\n",
      "epoch: 1600, accuracy: 0.723,               loss: 0.597, lr: 0.862\n",
      "epoch: 1700, accuracy: 0.743,               loss: 0.583, lr: 0.855\n",
      "epoch: 1800, accuracy: 0.743,               loss: 0.576, lr: 0.848\n",
      "epoch: 1900, accuracy: 0.743,               loss: 0.571, lr: 0.840\n",
      "epoch: 2000, accuracy: 0.737,               loss: 0.565, lr: 0.833\n",
      "epoch: 2100, accuracy: 0.743,               loss: 0.558, lr: 0.827\n",
      "epoch: 2200, accuracy: 0.743,               loss: 0.553, lr: 0.820\n",
      "epoch: 2300, accuracy: 0.750,               loss: 0.548, lr: 0.813\n",
      "epoch: 2400, accuracy: 0.750,               loss: 0.544, lr: 0.807\n",
      "epoch: 2500, accuracy: 0.763,               loss: 0.541, lr: 0.800\n",
      "epoch: 2600, accuracy: 0.773,               loss: 0.536, lr: 0.794\n",
      "epoch: 2700, accuracy: 0.790,               loss: 0.532, lr: 0.787\n",
      "epoch: 2800, accuracy: 0.797,               loss: 0.529, lr: 0.781\n",
      "epoch: 2900, accuracy: 0.797,               loss: 0.526, lr: 0.775\n",
      "epoch: 3000, accuracy: 0.793,               loss: 0.523, lr: 0.769\n",
      "epoch: 3100, accuracy: 0.797,               loss: 0.521, lr: 0.763\n",
      "epoch: 3200, accuracy: 0.797,               loss: 0.518, lr: 0.758\n",
      "epoch: 3300, accuracy: 0.800,               loss: 0.516, lr: 0.752\n",
      "epoch: 3400, accuracy: 0.810,               loss: 0.514, lr: 0.746\n",
      "epoch: 3500, accuracy: 0.803,               loss: 0.512, lr: 0.741\n",
      "epoch: 3600, accuracy: 0.803,               loss: 0.509, lr: 0.735\n",
      "epoch: 3700, accuracy: 0.810,               loss: 0.507, lr: 0.730\n",
      "epoch: 3800, accuracy: 0.807,               loss: 0.506, lr: 0.725\n",
      "epoch: 3900, accuracy: 0.803,               loss: 0.504, lr: 0.719\n",
      "epoch: 4000, accuracy: 0.793,               loss: 0.502, lr: 0.714\n",
      "epoch: 4100, accuracy: 0.793,               loss: 0.500, lr: 0.709\n",
      "epoch: 4200, accuracy: 0.793,               loss: 0.499, lr: 0.704\n",
      "epoch: 4300, accuracy: 0.797,               loss: 0.498, lr: 0.699\n",
      "epoch: 4400, accuracy: 0.793,               loss: 0.496, lr: 0.694\n",
      "epoch: 4500, accuracy: 0.797,               loss: 0.494, lr: 0.690\n",
      "epoch: 4600, accuracy: 0.800,               loss: 0.492, lr: 0.685\n",
      "epoch: 4700, accuracy: 0.800,               loss: 0.491, lr: 0.680\n",
      "epoch: 4800, accuracy: 0.800,               loss: 0.486, lr: 0.676\n",
      "epoch: 4900, accuracy: 0.797,               loss: 0.484, lr: 0.671\n",
      "epoch: 5000, accuracy: 0.793,               loss: 0.483, lr: 0.667\n",
      "epoch: 5100, accuracy: 0.797,               loss: 0.481, lr: 0.662\n",
      "epoch: 5200, accuracy: 0.783,               loss: 0.480, lr: 0.658\n",
      "epoch: 5300, accuracy: 0.787,               loss: 0.478, lr: 0.654\n",
      "epoch: 5400, accuracy: 0.790,               loss: 0.476, lr: 0.649\n",
      "epoch: 5500, accuracy: 0.790,               loss: 0.474, lr: 0.645\n",
      "epoch: 5600, accuracy: 0.803,               loss: 0.471, lr: 0.641\n",
      "epoch: 5700, accuracy: 0.803,               loss: 0.469, lr: 0.637\n",
      "epoch: 5800, accuracy: 0.803,               loss: 0.467, lr: 0.633\n",
      "epoch: 5900, accuracy: 0.803,               loss: 0.466, lr: 0.629\n",
      "epoch: 6000, accuracy: 0.797,               loss: 0.465, lr: 0.625\n",
      "epoch: 6100, accuracy: 0.797,               loss: 0.464, lr: 0.621\n",
      "epoch: 6200, accuracy: 0.800,               loss: 0.462, lr: 0.617\n",
      "epoch: 6300, accuracy: 0.800,               loss: 0.461, lr: 0.614\n",
      "epoch: 6400, accuracy: 0.800,               loss: 0.460, lr: 0.610\n",
      "epoch: 6500, accuracy: 0.800,               loss: 0.458, lr: 0.606\n",
      "epoch: 6600, accuracy: 0.800,               loss: 0.457, lr: 0.602\n",
      "epoch: 6700, accuracy: 0.800,               loss: 0.456, lr: 0.599\n",
      "epoch: 6800, accuracy: 0.800,               loss: 0.455, lr: 0.595\n",
      "epoch: 6900, accuracy: 0.800,               loss: 0.454, lr: 0.592\n",
      "epoch: 7000, accuracy: 0.807,               loss: 0.452, lr: 0.588\n",
      "epoch: 7100, accuracy: 0.807,               loss: 0.451, lr: 0.585\n",
      "epoch: 7200, accuracy: 0.807,               loss: 0.450, lr: 0.581\n",
      "epoch: 7300, accuracy: 0.807,               loss: 0.449, lr: 0.578\n",
      "epoch: 7400, accuracy: 0.813,               loss: 0.448, lr: 0.575\n",
      "epoch: 7500, accuracy: 0.820,               loss: 0.447, lr: 0.571\n",
      "epoch: 7600, accuracy: 0.820,               loss: 0.446, lr: 0.568\n",
      "epoch: 7700, accuracy: 0.820,               loss: 0.445, lr: 0.565\n",
      "epoch: 7800, accuracy: 0.820,               loss: 0.444, lr: 0.562\n",
      "epoch: 7900, accuracy: 0.820,               loss: 0.444, lr: 0.559\n",
      "epoch: 8000, accuracy: 0.813,               loss: 0.443, lr: 0.556\n",
      "epoch: 8100, accuracy: 0.823,               loss: 0.442, lr: 0.553\n",
      "epoch: 8200, accuracy: 0.823,               loss: 0.441, lr: 0.549\n",
      "epoch: 8300, accuracy: 0.823,               loss: 0.440, lr: 0.546\n",
      "epoch: 8400, accuracy: 0.823,               loss: 0.440, lr: 0.544\n",
      "epoch: 8500, accuracy: 0.823,               loss: 0.438, lr: 0.541\n",
      "epoch: 8600, accuracy: 0.830,               loss: 0.438, lr: 0.538\n",
      "epoch: 8700, accuracy: 0.827,               loss: 0.437, lr: 0.535\n",
      "epoch: 8800, accuracy: 0.827,               loss: 0.436, lr: 0.532\n",
      "epoch: 8900, accuracy: 0.827,               loss: 0.436, lr: 0.529\n",
      "epoch: 9000, accuracy: 0.830,               loss: 0.435, lr: 0.526\n",
      "epoch: 9100, accuracy: 0.833,               loss: 0.434, lr: 0.524\n",
      "epoch: 9200, accuracy: 0.833,               loss: 0.434, lr: 0.521\n",
      "epoch: 9300, accuracy: 0.833,               loss: 0.433, lr: 0.518\n",
      "epoch: 9400, accuracy: 0.830,               loss: 0.432, lr: 0.515\n",
      "epoch: 9500, accuracy: 0.833,               loss: 0.432, lr: 0.513\n",
      "epoch: 9600, accuracy: 0.833,               loss: 0.431, lr: 0.510\n",
      "epoch: 9700, accuracy: 0.833,               loss: 0.431, lr: 0.508\n",
      "epoch: 9800, accuracy: 0.833,               loss: 0.430, lr: 0.505\n",
      "epoch: 9900, accuracy: 0.833,               loss: 0.429, lr: 0.503\n",
      "epoch: 10000, accuracy: 0.837,               loss: 0.429, lr: 0.500\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# set up the neural network\n",
    "dense_1 = Dense(2, 64)\n",
    "activation_1 = ActivationReLU()\n",
    "dense_2 = Dense(64, 3)\n",
    "loss_activation = SoftmaxCCE()\n",
    "optim = AdaGrad(decay=1e-4)\n",
    "# training over epochs\n",
    "for epoch in range(10001):\n",
    "    # perform forward pass\n",
    "    dense_1.forward(X)\n",
    "    activation_1.forward(dense_1.output)\n",
    "    dense_2.forward(activation_1.output)\n",
    "    loss = loss_activation.forward(dense_2.output, y)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(preds == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, accuracy: {accuracy:.3f}, \\\n",
    "              loss: {loss:.3f}, lr: {optim.current_lr:.3f}')\n",
    "    # perform backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense_2.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(dense_2.dinputs)\n",
    "    dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "    # update weights and biases\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(dense_1)\n",
    "    optim.update_params(dense_2)\n",
    "    optim.update_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "Similar to AdaGradm, Root Mean Sqared Propogation caculates an adaptive learning rate per parameter.\n",
    "\n",
    "cache = rho * cache + (1 - rho) * gradient ** 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 08:45:18) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "641ad957094f18d0cb605a4fd5aa0132e734626d963fea77e92b8bc6efc59ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
